{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Info 557 Graduate Project**\n",
        "\n",
        "Angelina Allen\n"
      ],
      "metadata": {
        "id": "aFJYuD0bedNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project follows a complete machine learning pipeline to predict whether specific emotions are present in a given text:\n",
        "\n",
        "1. **Data Loading**: CSV files are loaded using the Hugging Face datasets library.\n",
        "2. **Preprocessing**: Text is tokenized and labels are formatted for multi-label classification.\n",
        "3. **Model Training**: A neural network model uses embeddings, CNN, GRU, and attention. Then is trained using TensorFlow/Keras with early stopping and model checkpoints\n",
        "4. **Prediction**: The trained model generates predictions and saves them in a submission file.\n",
        "\n",
        "Below, we will walk through my code and ensure that all parts of the project are clear and understood."
      ],
      "metadata": {
        "id": "1aD70141gWt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "CwAk9M9fiWmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datasets\n",
        "import transformers"
      ],
      "metadata": {
        "id": "EyVNMJK3i31k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explanatory Data Analysis**\n",
        "\n",
        "This was done separately from the file added to the github.\n",
        "\n",
        "*   Checked dataset shape and column names\n",
        "*   Verified if any missing values\n",
        "*   Looked for duplicate rows\n",
        "*   Analyzed text length distribution to identify possible outliers\n",
        "*   Reviewed label distributions to understand class imbalance in hopes to help fix prediction score\n",
        "\n"
      ],
      "metadata": {
        "id": "47qa7pI4kQNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Basic structure\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "print(\"\\nColumn names:\\n\", df.columns)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "# Text length analysis\n",
        "df[\"text_length\"] = df[\"text\"].astype(str).apply(len)\n",
        "\n",
        "print(\"Text length stats:\\n\", df[\"text_length\"].describe())\n",
        "\n",
        "# Outlier check: very long or very short texts\n",
        "print(\"Shortest texts:\")\n",
        "print(df.nsmallest(5, \"text_length\")[[\"text\", \"text_length\"]])\n",
        "\n",
        "print(\"\\nLongest texts:\")\n",
        "print(df.nlargest(5, \"text_length\")[[\"text\", \"text_length\"]])"
      ],
      "metadata": {
        "id": "j23C-9dMkZ01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Layer**\n",
        "I constructed this custom global sum pool that sums all values across the time/sequence dimension for my attention. We learned about doing this in our lecture, but I also got help from Google.\n"
      ],
      "metadata": {
        "id": "TV3QCy5pmpbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSumPooling1D(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)"
      ],
      "metadata": {
        "id": "z17vTF7PnSrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization Functions and Dataset Loading**\n",
        "\n",
        "* Text inputs are tokenized using a pretrained tokenizer.  \n",
        "* Sequences are truncated or padded to a fixed length to ensure consistency.\n",
        "* Labels are converted into numerical multi-hot vectors for multi-label classification.\n",
        "\n",
        "I kept all these parts the same as the baseline. I figure this will be a safe way to process my data in my model correctly. I made sure to build my model to work with this given tokenizer and Hugging Face function.\n",
        "\n",
        " **train/dev split** validation strategy was used:\n",
        "\n",
        "* The training set was used to fit the model parameters.\n",
        "* A separate validation (dev) set was used for hyperparameter tuning and early stopping.\n",
        "* Early stopping was monitored using validation F1-score to prevent overfitting.\n",
        "\n",
        "This approach ensures that model performance is measured on unseen data."
      ],
      "metadata": {
        "id": "aCgRZyW5ncmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer from baseline\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
        "\n",
        "# loading dataset\n",
        "def load_datasets(train_path=\"train.csv\", dev_path=\"dev.csv\"):\n",
        "    # train/dev split\n",
        "    hf_dataset = datasets.load_dataset(\"csv\", data_files={\"train\": train_path, \"validation\": dev_path})\n",
        "    labels = hf_dataset[\"train\"].column_names[1:]\n",
        "\n",
        "    def gather_labels(example):\n",
        "        return {\"labels\": [float(example[l]) for l in labels]}\n",
        "\n",
        "    hf_dataset = hf_dataset.map(gather_labels)\n",
        "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
        "\n",
        "    train_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
        "        columns=\"input_ids\",\n",
        "        label_cols=\"labels\",\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "    dev_dataset = hf_dataset[\"validation\"].to_tf_dataset(\n",
        "        columns=\"input_ids\",\n",
        "        label_cols=\"labels\",\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    return train_dataset, dev_dataset, labels"
      ],
      "metadata": {
        "id": "IbFCAK4noFJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that the hyperparameters shown were not the only ones tested. I extensively tuned and adjusted the model to identify the combination that produced the best performance. The values presented reflect the final configuration that achieved the highest score."
      ],
      "metadata": {
        "id": "k2GhQMmnoLHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nueral Network Model**\n",
        "\n",
        "The model combines:\n",
        "\n",
        "* Creating a regularizer that adds L2 weight decay to reduce overfitting\n",
        "* An embedding layer to convert tokens into dense vectors\n",
        "* A spatial dropout that drops entire word embedding vectors to prevent overfitting\n",
        "* A convolutional layer to extract local n-gram features\n",
        "* A bidirectional GRU layer to capture context in both directions\n",
        "* An attention mechanism to focus on the most important tokens\n",
        "* Fully connected layers for classification to refine the learned feature\n",
        "* Then outputs one sigmoid neuron per label for multi-label classification"
      ],
      "metadata": {
        "id": "MDAj-CrcqeLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_labels):\n",
        "    # Adds L2 weight decay to reduce overfitting\n",
        "    regularizer = tf.keras.regularizers.L2(0.0005)\n",
        "\n",
        "    # inputs defines model input\n",
        "    inputs = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
        "\n",
        "    # Embedding converts token IDs into dense vectors\n",
        "    x = tf.keras.layers.Embedding(\n",
        "        input_dim=tokenizer.vocab_size,\n",
        "        output_dim=128,\n",
        "        input_length=MAX_LEN,\n",
        "        mask_zero=True,\n",
        "        embeddings_regularizer=regularizer\n",
        "    )(inputs)\n",
        "\n",
        "    # drops entire word embedding vectors to prevent overfitting\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    # CNN extracts local n-gram features and downsamples\n",
        "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Bidirectional Gru processes text forward and backward to capture context\n",
        "    x = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.GRU(\n",
        "            64,\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3,\n",
        "            return_sequences=True\n",
        "        )\n",
        "    )(x)\n",
        "\n",
        "    # Attention applies an attention mechanism that assigns importance weights to each token,\n",
        "    # reweights token representations accordingly, and aggregates them into a single sequence-level feature vector.\n",
        "    score = tf.keras.layers.Dense(1)(x)\n",
        "    weights = tf.keras.layers.Softmax(axis=1)(score)\n",
        "    x = tf.keras.layers.Multiply()([x, weights])\n",
        "    x = GlobalSumPooling1D()(x)\n",
        "\n",
        "    # Dense head applies a fully connected layer to refine learned features and uses\n",
        "    # dropout for regularization to reduce overfitting.\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer)(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    # output one sigmoid neuron per label for multi-label classification\n",
        "    outputs = tf.keras.layers.Dense(num_labels, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Build model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Uses the Adam optimizer with an exponentially decaying learning rate and\n",
        "    # gradient clipping to ensure stable and efficient training\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=1e-3,\n",
        "            decay_steps=2000,\n",
        "            decay_rate=0.95\n",
        "        ),\n",
        "        clipnorm=1.0\n",
        "    )\n",
        "\n",
        "    # Compiles the model using the Adam optimizer, binary cross-entropy loss,\n",
        "    # and a micro-averaged F1 score metric with a fixed threshold.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=0.4)]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "tlXfuBqasWnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Method**\n",
        "\n",
        "The model is trained using:\n",
        "* Binary Cross-Entropy loss for multi-label classification\n",
        "* Adam optimizer with learning rate decay\n",
        "* Gradient clipping to stabilize training\n",
        "* Early stopping to prevent overfitting\n",
        "\n",
        "Validation F1-score is used to monitor performance."
      ],
      "metadata": {
        "id": "rG4g08XBsc9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model_path=\"model\", train_path=\"train.csv\", dev_path=\"dev.csv\"):\n",
        "    train_dataset, dev_dataset, labels = load_datasets(train_path, dev_path)\n",
        "    model = build_model(len(labels))\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=dev_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint( #Saves the best model\n",
        "                filepath=model_path + \".keras\",\n",
        "                monitor=\"val_f1\",\n",
        "                mode=\"max\",\n",
        "                save_best_only=True\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping( #Stops training early if validation F1 stops improving\n",
        "                monitor=\"val_f1\",\n",
        "                patience=3,\n",
        "                mode=\"max\",\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "pR_QMirxs_IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction Method**\n",
        "\n",
        "* Loads the saved trained model instead of training a new one.\n",
        "* Now uses test text only.\n",
        "* Converts model probability outputs into binary predictions using a 0.4 threshold.\n",
        "* Inserts the predictions back into the test dataframe.\n",
        "* Saves everything as submission.zip format.\n",
        "* Includes a small command-line interface so you can run train or predict from the terminal.\n",
        "\n",
        "I kept this close to the baseline model. The main change I made was tuning the decision threshold used to convert predicted probabilities into binary class labels, ultimately selecting 0.4. I experimented with several thresholds to improve performance on lower-frequency emotions, but 0.4 provided the best overall results."
      ],
      "metadata": {
        "id": "WeTZnrvotErQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# close to the same as baseline just update to fit mine\n",
        "def predict(model_path=\"model.keras\", input_path=\"test-ref.csv\"):\n",
        "    model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        compile=False,\n",
        "        custom_objects={\"GlobalSumPooling1D\": GlobalSumPooling1D})\n",
        "    df = pd.read_csv(input_path)\n",
        "    labels = df.columns[1:]\n",
        "\n",
        "    enc = tokenizer(df[\"text\"].tolist(), truncation=True, max_length=MAX_LEN, padding=\"max_length\", return_tensors=\"tf\")\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(enc[\"input_ids\"]).batch(BATCH_SIZE)\n",
        "\n",
        "    preds = model.predict(tf_dataset)\n",
        "    preds = (preds > 0.4).astype(int)  # Converts predicted probabilities into binary class labels using a 0.4 decision threshold\n",
        "\n",
        "    df.iloc[:, 1:] = preds\n",
        "    df.to_csv(\"submission.zip\", index=False, compression=dict(\n",
        "        method='zip', archive_name='submission.csv'\n",
        "    ))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"command\", choices={\"train\", \"predict\"})\n",
        "    args = parser.parse_args()\n",
        "    globals()[args.command]()"
      ],
      "metadata": {
        "id": "y9dYGaDVtITm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project File Structure**\n",
        "\n",
        "The project uses the following structure:\n",
        "\n",
        "- `train.csv`: Training dataset\n",
        "- `dev.csv`: Validation dataset\n",
        "- `test-ref.csv`: Test dataset\n",
        "- `model.keras`: Saved trained model\n",
        "- `submission.zip`: ZIP file containing the final predictions\n",
        "\n",
        "## **Submission Process**\n",
        "\n",
        "1. The trained model is loaded using TensorFlow.\n",
        "2. The dev/test data is tokenized using the same tokenizer as during training.\n",
        "3. The model generates probability predictions.\n",
        "4. Probabilities are converted into binary labels using a fixed threshold (0.4).\n",
        "5. Predictions are saved into `submission.csv`.\n",
        "6. The CSV file is compressed into `submission.zip` for submission.\n",
        "\n",
        "## **Model Evaluation**\n",
        "\n",
        "The model showed a micro F1 score of .83 on the dev set and a .82 on the test set. Compared to the baseline score of 0.75, I believe this indicates a noticeable improvement. Also, good generalization from the fact that the scores did not go down on the test dataset as much. Overall, the model performed well, especially given the challenges of multi-label classification with low-frequency emotions.\n",
        "\n",
        "## **Key Observations**\n",
        "* Adding the attention mechanism allowed the model to focus on important tokens more effectively, though low-frequency emotions remain challenging.\n",
        "* Dropout and L2 regularization reduced overfitting.\n",
        "* Hyperparameter tuning improved stability and convergence. This part was the most tendious but the most rewarding when you found the right balance.\n",
        "\n",
        "## **What I Learned**\n",
        "This project helped me understand how deep learning models process text and how regularization and validation strategies improve generalization. It showed me all the work that goes into making deep learning model and how hard it is to achieve a perfect score. I learned that progress comes from making incremental improvements rather than aiming for perfection. Also, the importance of generalization ensuring a model performs well not just on a specific dataset, but across other datasets and real-world applications.\n",
        "\n",
        "## **Future Improvements**\n",
        "\n",
        "In the future, I would love to really focus on how I can get a better performance for the low-frequency emotions. Possibly using a better transformer that has build in launguage knowlege, experimenting with weighted classes, or cross-validation for more robust evaluation. Overall, I learned a lot from this project and am excited to continue working with deep learning models."
      ],
      "metadata": {
        "id": "nE5PrVllJJy-"
      }
    }
  ]
}